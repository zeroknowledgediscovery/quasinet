{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:56:32.190666Z",
     "start_time": "2020-06-27T13:56:32.187034Z"
    }
   },
   "outputs": [],
   "source": [
    "NUMCPUS = 1\n",
    "\n",
    "# MAX_COLS = 20\n",
    "MAX_COLS = None\n",
    "\n",
    "# MAX_ROWS = 10\n",
    "MAX_ROWS = None\n",
    "\n",
    "TRAIN_QNETS = False\n",
    "\n",
    "COMPUTE_QDISTS = False\n",
    "\n",
    "COMPUTE_COMMON_STRAIN = True\n",
    "\n",
    "TYPE = 'h1n1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:56:32.380212Z",
     "start_time": "2020-06-27T13:56:32.227391Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = '/home/jinli11/quasinet/data/influenza/trees/h1n1humanHA/'\n",
    "DATA_DIR = '/project2/ishanu/hiv-dip/influenza/trees/h1n1humanHA/'\n",
    "\n",
    "OUTPUT_DIR = 'output/'\n",
    "\n",
    "INFLUENZA_OUT_DIR = OUTPUT_DIR + 'influenza/'\n",
    "\n",
    "INFLUENZA_QNET_DIR = INFLUENZA_OUT_DIR + 'qnets/'\n",
    "\n",
    "INFLUENZA_QDIST_DIR = INFLUENZA_OUT_DIR + 'qdistances/'\n",
    "\n",
    "HUMAN_HA_YEARLY_DIST_MATRIX_LDISTANCE_DIR = '/project2/ishanu/hiv-dip/influenza/output/yearly_distance_matrices_ldistance/h1n1humanHA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:06.115124Z",
     "start_time": "2020-06-27T13:56:32.382904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'compute_common_strain' from '/project2/ishanu/hiv-dip/quasinet/ntb/compute_common_strain.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "\n",
    "# sys.path.insert(1, '/home/jinli11/quasinet/quasinet/citrees/')\n",
    "sys.path.insert(1, '/project2/ishanu/hiv-dip/quasinet/quasinet/citrees/')\n",
    "\n",
    "# import citrees\n",
    "import qnet\n",
    "import tree\n",
    "\n",
    "import compute_common_strain\n",
    "from compute_common_strain import *\n",
    "reload(compute_common_strain)\n",
    "# reload(citrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `jupyter nbconvert --to script influenza.ipynb`\n",
    "* TODO: I need to clean the sequences to reduce redudant sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:06.129486Z",
     "start_time": "2020-06-27T13:57:06.117716Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_csv_files(dir_, index_col=None):\n",
    "    f_to_data = {}\n",
    "    for file_ in glob.glob(dir_ + '*.csv'):\n",
    "        f = os.path.basename(file_)\n",
    "        f = re.findall(r'\\d+_\\d+', f)[0]\n",
    "        \n",
    "        f_to_data[f] = pd.read_csv(file_, index_col=index_col)\n",
    "        \n",
    "    return f_to_data\n",
    "\n",
    "    \n",
    "def make_dir(dir_):\n",
    "    \"\"\"Make a directory if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        dir (str): directory to make\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(dir_):\n",
    "        os.makedirs(dir_)\n",
    "        \n",
    "def load_pickled(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_pickled(item, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(item, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def remove_index_and_cols(f_to_df):\n",
    "    \n",
    "    new_f_to_df = {}\n",
    "    for f, df in f_to_df.items():\n",
    "        df = df.copy(deep=True)\n",
    "        df.index = np.arange(0, df.shape[0])\n",
    "        df.columns = np.arange(0, df.shape[1])\n",
    "        \n",
    "        new_f_to_df[f] = df\n",
    "        \n",
    "    return new_f_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Making Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:06.143393Z",
     "start_time": "2020-06-27T13:57:06.131815Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_dir(INFLUENZA_OUT_DIR)\n",
    "make_dir(INFLUENZA_QNET_DIR)\n",
    "make_dir(INFLUENZA_QDIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:06.188998Z",
     "start_time": "2020-06-27T13:57:06.145940Z"
    }
   },
   "outputs": [],
   "source": [
    "def year_to_cleaned_sequence_data(sequence_data):\n",
    "    \"\"\"Basically clean the data so we can use it for computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for year, seq_data_ in sequence_data.items():\n",
    "        \n",
    "        seq_data = seq_data_.copy()\n",
    "        num_seqs, num_cols = seq_data.shape\n",
    "        \n",
    "        seq_data.drop_duplicates(inplace=True, subset=np.arange(0, num_cols).astype(str))\n",
    "        seq_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        data[year] = seq_data\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.284221Z",
     "start_time": "2020-06-27T13:57:06.191538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ha max length:  550\n"
     ]
    }
   ],
   "source": [
    "human_ha_seqs = load_csv_files(DATA_DIR)\n",
    "human_ha_max_len = list(human_ha_seqs.values())[0].shape[1]\n",
    "print (\"ha max length: \", human_ha_max_len)\n",
    "human_ha_cleaned_seqs = year_to_cleaned_sequence_data(human_ha_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qnet Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.292662Z",
     "start_time": "2020-06-27T13:57:12.286645Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_qnet(f, seqs, output_dir, max_cols, numCPUs):\n",
    "    seqs = seqs.values.astype(str)[:, :max_cols]\n",
    "    myqnet = qnet.Qnet(n_jobs=numCPUs)\n",
    "    myqnet.fit(seqs)\n",
    "\n",
    "#     basename = f.replace('.csv', '.joblib')\n",
    "    basename = f + '.joblib'\n",
    "    if output_dir is not None:\n",
    "        outfile = os.path.join(output_dir, basename)\n",
    "#         print (outfile)\n",
    "        qnet.save_qnet(myqnet, outfile)\n",
    "        \n",
    "    return basename, myqnet\n",
    "            \n",
    "def train_qnets(f_to_seqs, numCPUs, output_dir, max_cols=None):\n",
    "        \n",
    "    keys = list(f_to_seqs.keys())#[:2]\n",
    "    \n",
    "    base_name_trees = Parallel(n_jobs=numCPUs, backend='loky')(\n",
    "        delayed(train_qnet)(\n",
    "            key, f_to_seqs[key], output_dir, max_cols, numCPUs)\n",
    "        for key in keys\n",
    "        )\n",
    "    \n",
    "    f_to_qnet = {basename: tree for basename, tree in base_name_trees}\n",
    "    return f_to_qnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qnet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.303251Z",
     "start_time": "2020-06-27T13:57:12.294740Z"
    }
   },
   "outputs": [],
   "source": [
    "# result = train_qnet(\n",
    "#     'h1n1human2009_2010.csv',\n",
    "#     f_to_seqs['h1n1human2009_2010.csv'], \n",
    "#     output_dir=None, \n",
    "#     max_cols=MAX_COLS, \n",
    "#     numCPUs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.311041Z",
     "start_time": "2020-06-27T13:57:12.305133Z"
    }
   },
   "outputs": [],
   "source": [
    "if TRAIN_QNETS:\n",
    "    f_to_qnets = train_qnets(\n",
    "        human_ha_cleaned_seqs, \n",
    "        numCPUs=NUMCPUS, \n",
    "        output_dir=INFLUENZA_QNET_DIR, \n",
    "        max_cols=MAX_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDistance Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:00:09.329693Z",
     "start_time": "2020-06-27T14:00:09.316602Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_qnets(dir_):\n",
    "    f_to_qnets = {}\n",
    "    for f in glob.glob(dir_ + \"*.joblib\"):\n",
    "        f_to_qnets[os.path.basename(f)] = qnet.load_qnet(f)\n",
    "        \n",
    "    return f_to_qnets\n",
    "\n",
    "def compute_qdist(f, seqs, myqnet, max_seqs, max_cols):\n",
    "    seqs = seqs.drop_duplicates()\n",
    "    indices = seqs.index[:max_seqs]\n",
    "    seqs = seqs.values\n",
    "\n",
    "    dm = qnet.qdistance_matrix(\n",
    "        seqs[:max_seqs, :max_cols], \n",
    "        seqs[:max_seqs, :max_cols],\n",
    "        myqnet, \n",
    "        myqnet)\n",
    "\n",
    "    dm = pd.DataFrame(dm, index=indices, columns=indices)\n",
    "\n",
    "    return f, dm\n",
    "    \n",
    "def compute_qdists(f_to_qnets, f_to_seqs, max_seqs, max_cols, numCPUs, outdir):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    keys = list(f_to_seqs.keys())#[:2]\n",
    "    qnet_names = [key.split('.')[0] + '.joblib' for key in keys]\n",
    "    \n",
    "    f_dms = Parallel(n_jobs=numCPUs, backend='loky')(\n",
    "        delayed(compute_qdist)(\n",
    "            key, f_to_seqs[key], f_to_qnets[qnet_names[i]], max_seqs, max_cols)\n",
    "        for i, key in enumerate(keys)\n",
    "        )\n",
    "    \n",
    "    f_to_dms = {}\n",
    "    for f, dm in f_dms:\n",
    "        f_to_dms[f] = dm\n",
    "        dm.to_csv(os.path.join(outdir, f + '.csv'))\n",
    "#     f_to_dms = {f: dm for f, dm in f_dms}\n",
    "    \n",
    "    return f_to_dms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.331112Z",
     "start_time": "2020-06-27T13:57:12.326010Z"
    }
   },
   "outputs": [],
   "source": [
    "# f_to_qnets = load_qnets(INFLUENZA_QNET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.337143Z",
     "start_time": "2020-06-27T13:57:12.332680Z"
    }
   },
   "outputs": [],
   "source": [
    "# sys.getsizeof(f_to_qnets['h1n1human2000_2001.joblib'].estimators_) #.estimators_[1].root\n",
    "# tree.get_nodes(f_to_qnets['h1n1human2000_2001.joblib'].estimators_[2].root)\n",
    "# save_pickled(f_to_qnets['h1n1human2000_2001.joblib'], 'TMP.pkl')\n",
    "# save_pickled(f_to_qnets['h1n1human2000_2001.joblib'].estimators_, 'TMP2.pkl')\n",
    "# save_pickled(f_to_qnets['h1n1human2000_2001.joblib'].estimators_[0], 'TMP3.pkl')\n",
    "# f_to_qnets['h1n1human2000_2001.joblib'].estimators_[0].feature_importances_.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.367246Z",
     "start_time": "2020-06-27T13:57:12.339078Z"
    }
   },
   "outputs": [],
   "source": [
    "if COMPUTE_QDISTS:\n",
    "    qdist = compute_qdists(\n",
    "        f_to_qnets, human_ha_seqs, max_seqs=MAX_ROWS, \n",
    "        max_cols=MAX_COLS, numCPUs=NUMCPUS ** 2, outdir=INFLUENZA_QDIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:02:30.364534Z",
     "start_time": "2020-06-27T14:02:26.317453Z"
    }
   },
   "outputs": [],
   "source": [
    "human_ha_qdistance_dm = load_csv_files(INFLUENZA_QDIST_DIR, index_col=0)\n",
    "human_ha_ldistance_dm = load_csv_files(HUMAN_HA_YEARLY_DIST_MATRIX_LDISTANCE_DIR, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:02:50.919838Z",
     "start_time": "2020-06-27T14:02:50.916510Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(human_ha_qdistance_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Common Strain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compute Dominant Strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:23:02.582810Z",
     "start_time": "2020-06-27T14:23:02.562522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_dominant_strain(\n",
    "    name_to_dm, n_clusters, \n",
    "    upper_diag,\n",
    "    file_to_seqs=None, \n",
    "    remove_outliers=False,\n",
    "    clustering_method='agg',\n",
    "    min_type='average'):\n",
    "    \"\"\"Seperate the sequences into clusters, find the largest cluster, and find the \n",
    "    centroid of that largest cluster.\n",
    "    \n",
    "    Args:\n",
    "        name_to_dm (dict): mapping file name to distance matrix\n",
    "        use_accession (bool): whether we are using the accession or not\n",
    "        min_type (str): which type to use to compute the minimum\n",
    "        remove_outliers (bool): whether to remove outliers. N\n",
    "    \"\"\"\n",
    "    \n",
    "    dominant_strains = []\n",
    "    names = []\n",
    "    seqs = []\n",
    "    accession_names = []\n",
    "    for name_, dm in name_to_dm.items():\n",
    "        name = name_\n",
    "#         import pdb; pdb.set_trace()\n",
    "        dm.fillna(0, inplace=True)\n",
    "        names.append(name)\n",
    "        dm.columns = dm.columns.astype(str)\n",
    "        dm.index = dm.index.astype(str)\n",
    "        \n",
    "        #if dm.shape[0] == dm.shape[1]:\n",
    "        if upper_diag:\n",
    "            dm = dm.values + dm.T.values\n",
    "            \n",
    "        #dm = pd.DataFrame(dm, columns=columns, index=index)\n",
    "        \n",
    "        if remove_outliers:\n",
    "            dm = remove_outliers_func(dm)\n",
    "            \n",
    "        if min_type in ['average', 'median']:\n",
    "            if n_clusters == 1:\n",
    "                sub_dm = dm\n",
    "            else:\n",
    "                clusters = find_clusters(\n",
    "                    dm, n_clusters=n_clusters, \n",
    "                    cluster_type=clustering_method)\n",
    "                \n",
    "                sub_dm = find_largest_cluster_dm(dm, clusters)\n",
    "\n",
    "            if dm.shape == (1, 1):\n",
    "                dominant_strain = dm.index[0]\n",
    "            else:\n",
    "                if min_type == 'average':\n",
    "                    aggregated = sub_dm.sum(axis=1)\n",
    "                elif min_type == 'median':\n",
    "                    aggregated = sub_dm.median(axis=1)\n",
    "                else:\n",
    "                    raise ValueError\n",
    "                    \n",
    "#                 import pdb; pdb.set_trace()\n",
    "                dominant_strain = aggregated.idxmin()\n",
    "                #dominant_strain = np.argsort(aggregated)[len(aggregated)//2]\n",
    "                \n",
    "        elif min_type == 'normal':\n",
    "            \n",
    "            embedding = MDS(\n",
    "                n_components=1, \n",
    "                dissimilarity=\"precomputed\", \n",
    "                random_state=42)\n",
    "            \n",
    "            embed = embedding.fit_transform(dm)[:,0]\n",
    "            \n",
    "            dominant_strain = dm.index[np.argsort(embed)[len(embed)//2]]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Not a correct type: {}'.format(min_type))\n",
    "\n",
    "        dominant_strains.append(dominant_strain)\n",
    "        \n",
    "        \n",
    "        assert file_to_seqs is not None\n",
    "\n",
    "        try:\n",
    "            dominant_strain = int(dominant_strain)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        seq = ''.join(file_to_seqs[name].iloc[dominant_strain])\n",
    "        accession_name = '____'\n",
    "           \n",
    "        accession_names.append(accession_name)\n",
    "        seqs.append(seq)\n",
    "        \n",
    "        \n",
    "    data = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'dominant_strains': dominant_strains,\n",
    "        'sequence': seqs,\n",
    "        'accession_name': accession_names\n",
    "    })\n",
    "    \n",
    "    data.sort_values(by='name', inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Merge Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T13:57:12.410944Z",
     "start_time": "2020-06-27T13:57:12.394979Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_ldistances(seqs1, seqs2, max_size=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    NOTE: if seq1 or seq2 is blank, we return -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(seqs1) == len(seqs2)\n",
    "    \n",
    "    ldist = []\n",
    "    for i, seq1 in enumerate(seqs1):\n",
    "        seq2 = seqs2[i]\n",
    "        \n",
    "        if seq1 == '-1' or seq2 == '-1':\n",
    "            dist = -1\n",
    "        else:\n",
    "            dist = Levenshtein.distance(seq1[:max_size], seq2[:max_size])\n",
    "            \n",
    "        ldist.append(dist)\n",
    "        \n",
    "    return ldist\n",
    "\n",
    "\n",
    "def merge_prediction_data(\n",
    "    WHO_rec, \n",
    "    qnet_rec, dominant_strains, subtype, \n",
    "    f_to_seqs,\n",
    "    outfile=None,\n",
    "    max_size=None):\n",
    "    \n",
    "    WHO_rec = WHO_rec.reset_index(drop=True)\n",
    "    qnet_rec = qnet_rec.reset_index(drop=True)\n",
    "    dominant_strains = dominant_strains.reset_index(drop=True)\n",
    "    \n",
    "    data = pd.DataFrame({'year': WHO_rec['year'].values})\n",
    "    data['WHO_recommendation_name'] = WHO_rec['name']\n",
    "    data['WHO_recommendation_sequence'] = WHO_rec[subtype]\n",
    "    \n",
    "    data['dominant_strain_accession'] = list(dominant_strains['dominant_strains'].values[1:]) + ['-1']\n",
    "    data['dominant_strain_sequence'] = list(dominant_strains['sequence'].values[1:]) + ['-1']\n",
    "    \n",
    "    dom_strain_acc_name = list(dominant_strains['accession_name'].values[1:])\n",
    "#     dom_strain_acc_name = list(map(parse_influenza_name, dom_strain_acc_name))\n",
    "    dom_strain_acc_name = dom_strain_acc_name + ['-1']\n",
    "    data['dominant_strain_accession_name'] = dom_strain_acc_name\n",
    "    \n",
    "    data['qdistance_recommendation_accession'] = qnet_rec['dominant_strains']\n",
    "    data['qdistance_recommendation_sequence'] = qnet_rec['sequence']\n",
    "    \n",
    "#     qdist_acc_name = list(map(parse_influenza_name, qnet_rec['accession_name']))\n",
    "    qdist_acc_name = qnet_rec['accession_name']\n",
    "    data['qdistance_recommendation_accession_name'] = qdist_acc_name\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    ldistance_WHO = compute_ldistances(\n",
    "        data['WHO_recommendation_sequence'][:-1],\n",
    "        data['dominant_strain_sequence'][:-1],\n",
    "        max_size=max_size)\n",
    "    \n",
    "    ldistance_qnet_rec = compute_ldistances(\n",
    "        data['qdistance_recommendation_sequence'][:-1],\n",
    "        data['dominant_strain_sequence'][:-1],\n",
    "        max_size=max_size)\n",
    "    \n",
    "    data['ldistance_WHO'] = ldistance_WHO + [-1]\n",
    "    data['ldistance_Qnet_recommendation'] = ldistance_qnet_rec + [-1]\n",
    "    \n",
    "    num_rows = data.shape[0]\n",
    "    num_samples = []\n",
    "    for i in range(num_rows):\n",
    "        year_range = data['year'].iloc[i]\n",
    "        year = year_range.split('_')[0]\n",
    "        if year_range in f_to_seqs:\n",
    "            num_samples.append(f_to_seqs[year_range].shape[0])\n",
    "        else:\n",
    "            num_samples.append(-1)\n",
    "            \n",
    "    data['qnet_sample_size'] = num_samples\n",
    "            \n",
    "    if outfile is not None:\n",
    "        data.to_csv(outfile, index=None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:02:52.861309Z",
     "start_time": "2020-06-27T14:02:52.857642Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 1\n",
    "NUM_CLUSTERS_QDIST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:06:50.657694Z",
     "start_time": "2020-06-27T14:06:50.191465Z"
    }
   },
   "outputs": [],
   "source": [
    "if COMPUTE_COMMON_STRAIN:\n",
    "\n",
    "#     ha_dominant_strains_ldist = compute_dominant_strain(\n",
    "#         remove_index_and_cols(human_ha_ldistance_dm),\n",
    "#         NUM_CLUSTERS,\n",
    "#         None,\n",
    "#         HUMAN_HA_YEARLY_DIST_MATRIX_LDISTANCE_DIR,\n",
    "#         use_accession=False, \n",
    "#         file_to_seqs=human_ha_seqs, \n",
    "#         file_to_seqs_base_dir='{}human'.format(TYPE, TYPE))\n",
    "    \n",
    "    ha_dominant_strains_ldist = compute_dominant_strain(\n",
    "        name_to_dm=remove_index_and_cols(human_ha_ldistance_dm), \n",
    "        upper_diag=True,\n",
    "        n_clusters=NUM_CLUSTERS, \n",
    "        file_to_seqs=human_ha_cleaned_seqs, \n",
    "        remove_outliers=False)\n",
    "\n",
    "    ha_dominant_strains_qdist = compute_dominant_strain(\n",
    "        name_to_dm=remove_index_and_cols(human_ha_qdistance_dm), \n",
    "        upper_diag=False,\n",
    "        n_clusters=1, \n",
    "        file_to_seqs=human_ha_cleaned_seqs, \n",
    "        remove_outliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:22:40.094042Z",
     "start_time": "2020-06-27T14:22:40.091366Z"
    }
   },
   "outputs": [],
   "source": [
    "# test = remove_index_and_cols(human_ha_qdistance_dm)['2009_2010']\n",
    "# pd.DataFrame(\n",
    "#     test,\n",
    "#     index=test.index.astype(str),\n",
    "#     columns=test.columns.astype(str))#.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:23:08.713214Z",
     "start_time": "2020-06-27T14:23:08.519615Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:23:10.361722Z",
     "start_time": "2020-06-27T14:23:10.358387Z"
    }
   },
   "outputs": [],
   "source": [
    "# human_ha_qdistance_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:23:10.533340Z",
     "start_time": "2020-06-27T14:23:10.525243Z"
    }
   },
   "outputs": [],
   "source": [
    "WHO_recommendations = pd.read_csv(\n",
    "    '/project2/ishanu/hiv-dip/influenza/data/WHO_recommendations_Northern_Hemisphere.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:28:07.560803Z",
     "start_time": "2020-06-27T14:28:07.532010Z"
    }
   },
   "outputs": [],
   "source": [
    "if COMPUTE_COMMON_STRAIN:\n",
    "    human_ha_max_len = 550\n",
    "    humanHA_recommendations = merge_prediction_data(\n",
    "        WHO_recommendations,\n",
    "        ha_dominant_strains_qdist,\n",
    "        ha_dominant_strains_ldist,\n",
    "        'HA_seq',\n",
    "        human_ha_cleaned_seqs,\n",
    "        outfile=None,\n",
    "        max_size=human_ha_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:23:42.830124Z",
     "start_time": "2020-06-27T14:23:42.827928Z"
    }
   },
   "outputs": [],
   "source": [
    "# human_ha_seqs['2008_2009'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:32:10.526466Z",
     "start_time": "2020-06-27T14:32:10.523196Z"
    }
   },
   "outputs": [],
   "source": [
    "# humanHA_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:32:24.178129Z",
     "start_time": "2020-06-27T14:32:24.167703Z"
    }
   },
   "outputs": [],
   "source": [
    "# ha_dominant_strains_qdist\n",
    "# ldists = compute_ldistances(\n",
    "#     humanHA_recommendations['qdistance_recommendation_sequence'].values, \n",
    "#     x['dominant_strain_sequence'], max_size=550)\n",
    "\n",
    "# humanHA_recommendations['ldistance_Qnet_recommendation'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:38:01.670836Z",
     "start_time": "2020-06-27T14:38:01.667475Z"
    }
   },
   "outputs": [],
   "source": [
    "# humanHA_recommendations['ldistance_WHO'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:29:11.420688Z",
     "start_time": "2020-06-27T14:29:11.389488Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = pd.read_csv('/project2/ishanu/hiv-dip/corona/influenza/recommendations/humanHA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T14:33:09.376254Z",
     "start_time": "2020-06-27T14:33:09.372859Z"
    }
   },
   "outputs": [],
   "source": [
    "# x['ldistance_Qnet_recommendation'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quasinet_new",
   "language": "python",
   "name": "quasinet_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
